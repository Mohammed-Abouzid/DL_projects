{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "kN1sqLpV1bqR",
    "outputId": "81341a62-f37d-4f3a-df18-77cb58fd8dd7"
   },
   "outputs": [],
   "source": [
    "#Source:   https://www.youtube.com/watch?v=DtBu1u5aBsc&ab_channel=NehaYadav\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6562
    },
    "colab_type": "code",
    "id": "jbrkxkK1LvY0",
    "outputId": "023e8d3e-4336-4388-9f9f-0887f8ecb6bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n",
    "\n",
    "df=pd.read_csv('fer2013.csv')   # the data sets file, contains train/tesr-sets,   i/p is image--> o/p is emotion\n",
    "\n",
    "#print(df.info())    #file/data info\n",
    "#print(df[\"Usage\"].value_counts()) # num of trainingset, testset/public test\n",
    "\n",
    "#print(df.head())   # the head of the columns: emotion & pixels/image\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.sample.data:[array([ 70.,  80.,  82., ..., 106., 109.,  82.], dtype=float32), array([151., 150., 147., ..., 193., 183., 184.], dtype=float32)]\n",
      "train_y.sample.data:[0, 0]\n",
      "X_test.sample.data:[array([254., 254., 254., ...,  42., 129., 180.], dtype=float32), array([156., 184., 198., ..., 172., 167., 161.], dtype=float32)]\n",
      "test_y.sample.data:[0, 1]\n"
     ]
    }
   ],
   "source": [
    "X_train,train_y,X_test,test_y=[],[],[],[]    #create empty lists for the stes, it will be filled later\n",
    "\n",
    "for index, row in df.iterrows():       #split the datasets and store them in the lists\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            X_train.append(np.array(val,'float32'))\n",
    "            train_y.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            X_test.append(np.array(val,'float32'))\n",
    "            test_y.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n",
    "\n",
    "\n",
    "print(f\"X_train.sample.data:{X_train[0:2]}\")\n",
    "print(f\"train_y.sample.data:{train_y[0:2]}\")\n",
    "print(f\"X_test.sample.data:{X_test[0:2]}\")\n",
    "print(f\"test_y.sample.data:{test_y[0:2]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "epochs = 30\n",
    "width, height = 48, 48\n",
    "\n",
    "#convert the lists to arrays with data_type=float32\n",
    "X_train = np.array(X_train,'float32')\n",
    "train_y = np.array(train_y,'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "test_y = np.array(test_y,'float32')\n",
    "\n",
    "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)\n",
    "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)\n",
    "\n",
    "#cannot produce\n",
    "#normalizing data between o and 1\n",
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_train /= np.std(X_train, axis=0)\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "X_test /= np.std(X_test, axis=0)\n",
    "\n",
    "#reshape the data\n",
    "X_train = X_train.reshape(X_train.shape[0], width, height, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], width, height, 1)\n",
    "\n",
    "# print(f\"shape:{X_train.shape}\")\n",
    "##designing the cnn\n",
    "#1st convolution layer\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(num_features, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv2D(num_features,kernel_size= (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(2*num_features, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(2*num_features, (3, 3), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(2*2*2*2*num_features, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#softmax\n",
    "model.add(Dense(num_labels, activation='softmax'))  #num of emotion=7\n",
    "\n",
    "\n",
    "\n",
    "# model.summary()\n",
    "\n",
    "#Compliling the model\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Abouzid\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/30\n",
      "28709/28709 [==============================] - 192s 7ms/step - loss: 1.7053 - accuracy: 0.3051 - val_loss: 1.5289 - val_accuracy: 0.4018\n",
      "Epoch 2/30\n",
      "28709/28709 [==============================] - 186s 6ms/step - loss: 1.5127 - accuracy: 0.4081 - val_loss: 1.4077 - val_accuracy: 0.4567\n",
      "Epoch 3/30\n",
      "28709/28709 [==============================] - 190s 7ms/step - loss: 1.4089 - accuracy: 0.4537 - val_loss: 1.3210 - val_accuracy: 0.4909\n",
      "Epoch 4/30\n",
      "28709/28709 [==============================] - 185s 6ms/step - loss: 1.3403 - accuracy: 0.4855 - val_loss: 1.3132 - val_accuracy: 0.4957\n",
      "Epoch 5/30\n",
      "28709/28709 [==============================] - 185s 6ms/step - loss: 1.2987 - accuracy: 0.4973 - val_loss: 1.2640 - val_accuracy: 0.5082\n",
      "Epoch 6/30\n",
      "28709/28709 [==============================] - 190s 7ms/step - loss: 1.2603 - accuracy: 0.5133 - val_loss: 1.2294 - val_accuracy: 0.5364\n",
      "Epoch 7/30\n",
      "28709/28709 [==============================] - 198s 7ms/step - loss: 1.2299 - accuracy: 0.5253 - val_loss: 1.2193 - val_accuracy: 0.5372\n",
      "Epoch 8/30\n",
      "28709/28709 [==============================] - 191s 7ms/step - loss: 1.2016 - accuracy: 0.5385 - val_loss: 1.2093 - val_accuracy: 0.5469\n",
      "Epoch 9/30\n",
      "28709/28709 [==============================] - 200s 7ms/step - loss: 1.1808 - accuracy: 0.5447 - val_loss: 1.2060 - val_accuracy: 0.5397\n",
      "Epoch 10/30\n",
      "28709/28709 [==============================] - 203s 7ms/step - loss: 1.1566 - accuracy: 0.5560 - val_loss: 1.1780 - val_accuracy: 0.5553\n",
      "Epoch 11/30\n",
      "28709/28709 [==============================] - 208s 7ms/step - loss: 1.1344 - accuracy: 0.5654 - val_loss: 1.1935 - val_accuracy: 0.5514\n",
      "Epoch 12/30\n",
      "28709/28709 [==============================] - 208s 7ms/step - loss: 1.1162 - accuracy: 0.5750 - val_loss: 1.1919 - val_accuracy: 0.5405\n",
      "Epoch 13/30\n",
      "28709/28709 [==============================] - 199s 7ms/step - loss: 1.0939 - accuracy: 0.5804 - val_loss: 1.1517 - val_accuracy: 0.5639\n",
      "Epoch 14/30\n",
      "28709/28709 [==============================] - 199s 7ms/step - loss: 1.0778 - accuracy: 0.5871 - val_loss: 1.1892 - val_accuracy: 0.5514\n",
      "Epoch 15/30\n",
      "28709/28709 [==============================] - 196s 7ms/step - loss: 1.0658 - accuracy: 0.5912 - val_loss: 1.1760 - val_accuracy: 0.5637\n",
      "Epoch 16/30\n",
      "28709/28709 [==============================] - 195s 7ms/step - loss: 1.0390 - accuracy: 0.6050 - val_loss: 1.1855 - val_accuracy: 0.5595\n",
      "Epoch 17/30\n",
      "28709/28709 [==============================] - 194s 7ms/step - loss: 1.0262 - accuracy: 0.6107 - val_loss: 1.1792 - val_accuracy: 0.5553\n",
      "Epoch 18/30\n",
      "28709/28709 [==============================] - 194s 7ms/step - loss: 1.0093 - accuracy: 0.6157 - val_loss: 1.1603 - val_accuracy: 0.5723\n",
      "Epoch 19/30\n",
      "28709/28709 [==============================] - 196s 7ms/step - loss: 0.9947 - accuracy: 0.6218 - val_loss: 1.1955 - val_accuracy: 0.5676\n",
      "Epoch 20/30\n",
      "28709/28709 [==============================] - 192s 7ms/step - loss: 0.9754 - accuracy: 0.6287 - val_loss: 1.1728 - val_accuracy: 0.5704\n",
      "Epoch 21/30\n",
      "28709/28709 [==============================] - 196s 7ms/step - loss: 0.9638 - accuracy: 0.6332 - val_loss: 1.1754 - val_accuracy: 0.5659\n",
      "Epoch 22/30\n",
      "28709/28709 [==============================] - 198s 7ms/step - loss: 0.9449 - accuracy: 0.6401 - val_loss: 1.1785 - val_accuracy: 0.5754\n",
      "Epoch 23/30\n",
      "28709/28709 [==============================] - 193s 7ms/step - loss: 0.9226 - accuracy: 0.6473 - val_loss: 1.1799 - val_accuracy: 0.5698\n",
      "Epoch 24/30\n",
      "28709/28709 [==============================] - 194s 7ms/step - loss: 0.9112 - accuracy: 0.6536 - val_loss: 1.2032 - val_accuracy: 0.5673\n",
      "Epoch 25/30\n",
      "28709/28709 [==============================] - 196s 7ms/step - loss: 0.8958 - accuracy: 0.6571 - val_loss: 1.2251 - val_accuracy: 0.5717\n",
      "Epoch 26/30\n",
      "28709/28709 [==============================] - 199s 7ms/step - loss: 0.8909 - accuracy: 0.6624 - val_loss: 1.2184 - val_accuracy: 0.5690\n",
      "Epoch 27/30\n",
      "28709/28709 [==============================] - 184s 6ms/step - loss: 0.8696 - accuracy: 0.6692 - val_loss: 1.2117 - val_accuracy: 0.5756\n",
      "Epoch 28/30\n",
      "28709/28709 [==============================] - 193s 7ms/step - loss: 0.8538 - accuracy: 0.6787 - val_loss: 1.2449 - val_accuracy: 0.5731\n",
      "Epoch 29/30\n",
      "28709/28709 [==============================] - 198s 7ms/step - loss: 0.8455 - accuracy: 0.6805 - val_loss: 1.2807 - val_accuracy: 0.5539\n",
      "Epoch 30/30\n",
      "28709/28709 [==============================] - 199s 7ms/step - loss: 0.8362 - accuracy: 0.6841 - val_loss: 1.2018 - val_accuracy: 0.5790\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training the model\n",
    "model.fit(X_train, train_y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, test_y),\n",
    "          shuffle=True)\n",
    "\n",
    "\n",
    "#Saving the  model to  use it later on\n",
    "fer_json = model.to_json()\n",
    "with open(\"fer.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save_weights(\"fer.h5\")             #save weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the trained model on cam\n",
    "\n",
    "cap= cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, test_im=cap.read() \n",
    "    test_img= cv2.flip(test_im, 1)\n",
    "\n",
    "    if not ret:\n",
    "        continue\n",
    "    gray_img= cv2.cvtColor(test_img, cv2.COLOR_BGR2GRAY)\n",
    "    faces_detected= face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)\n",
    "    \n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        cv2.rectangle(test_img, (x,y), (x+w, y+h), (255, 0, 0), thickness= 3)\n",
    "        roi_gray= gray_img[y:y+w,x:x+h]\n",
    "        roi_gray= cv2.resize(roi_gray, (48,48))\n",
    "        img_pixels= image.img_to_array(roi_gray)\n",
    "        img_pixels= np.expand_dims(img_pixels, axis=0)\n",
    "        img_pixels/=255     #normalize\n",
    "        \n",
    "        predictions= mdl.predict(img_pixels)\n",
    "        \n",
    "        #find max indexed array\n",
    "        max_index= np.argmax(predictions[0])\n",
    "        emotions= ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "        predicted_emotion= emotions[max_index]\n",
    "        \n",
    "        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 255), 1)\n",
    "        \n",
    "    resized_img= cv2.resize(test_img, (1000, 700))\n",
    "    cv2.imshow('Facial Emotion Recognition', resized_img)\n",
    "    \n",
    "    if cv2.waitKey(10)==ord('q'):\n",
    "        break\n",
    "cap.release\n",
    "cv2.destroyAllWindows()    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Emotion_Anaysis.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
